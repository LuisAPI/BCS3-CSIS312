{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*De La Salle University – Dasmariñas* \\\n",
        "*College of Information and Computer Studies*\n",
        "\n",
        "**S–CSIS312 — Natural Language Processing** \\\n",
        "Finals — Activity 2\n",
        "\n",
        "**Name:** Luis Anton P. Imperial \\\n",
        "**CYS:** BCS32"
      ],
      "metadata": {
        "id": "nnJkX3ALJEU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finals Programming Activity 2: Tokenization in NLP with Python\n",
        "\n",
        "## Case Scenario: Analyzing Customer Feedback for a Tech Company\n",
        "\n",
        "A tech company has received a large volume of customer feedback via surveys, social media\n",
        "posts, and support tickets. They need to analyze the feedback to identify common themes, key\n",
        "opinions, and issues raised by customers. To do so, the company decides to preprocess the text\n",
        "data and perform a basic analysis to understand the language used by customers.\n",
        "\n",
        "The first step in this analysis is tokenization—the process of breaking the feedback text into\n",
        "smaller units (tokens) such as words, phrases, and sentences. Tokenization is the foundational\n",
        "task that helps further analyze text, such as extracting features, building word clouds, or\n",
        "identifying sentiment.\n",
        "\n",
        "In this activity, you will implement tokenization techniques using Python to preprocess the\n",
        "customer feedback data.\n",
        "\n",
        "## Objectives:\n",
        "\n",
        "1. Use NLTK and spaCy to tokenize customer feedback.\n",
        "2. Perform sentence and word tokenization.\n",
        "3. Remove irrelevant words (stopwords) to focus on important information.\n",
        "4. Extract useful entities (like product names or issues) from the feedback.\n",
        "\n",
        "## Install Required Libraries:\n",
        "\n",
        "1. If you haven't installed nltk or spaCy, do so now: `!pip install nltk spacy`\n",
        "2. For spaCy, download the English model: `python -m spacy download en_core_web_sm`\n",
        "3. Download NLTK Resources\n",
        "  ```\n",
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "  nltk.download('stopwords')\n",
        "  ```"
      ],
      "metadata": {
        "id": "GnMc5sdPIntH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HRZhg3PHmjp",
        "outputId": "cf8acef6-4986-4d60-a51e-a1f2547a625b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.13.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPZEdELIKyeJ",
        "outputId": "2a7f3fe5-f07b-465f-b3b3-3fc372094e4a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Onhiop5NKo2Y",
        "outputId": "853fb9c3-bef2-4f9e-eeaf-ddd6e7cd52fb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Tokenizing Customer Feedback\n",
        "\n",
        "You will start by tokenizing some customer feedback text into words and sentences.\n",
        "\n",
        "### Scenario Example:\n",
        "\n",
        "Customer feedback text:\n",
        "\n",
        "\"Great product, but the software crashed twice in the last week. The customer support team was very helpful, though. Could improve the battery life.\"\n",
        "\n",
        "a. **Tokenize the Feedback into Sentences and Words using NLTK** (Write functions to split the feedback into sentences and words using the NLTK library)\n",
        "  - Sample Output:\n",
        "\n",
        "b. **Tokenize the Feedback into Words using spaCy** (use spaCy to tokenize the same feedback into word)"
      ],
      "metadata": {
        "id": "ETmXcL2AKuPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_feedback():\n",
        "    feedbacks = [\n",
        "        \"The phone's camera quality is amazing!\",\n",
        "        \"I'm having trouble connecting to the Wi-Fi network.\",\n",
        "        \"The battery life of my new Galaxy S23 is impressive.\",\n",
        "        \"Customer service was helpful, resolved the issue quickly.\",\n",
        "        \"Disappointed with the latest update, it's buggy.\",\n",
        "        \"The new features in iOS 16 are fantastic, especially the improved focus mode.\",\n",
        "        \"The screen of my iPhone 14 is quite bright.\",\n",
        "        \"I had a problem with the software, the support team from Apple was helpful but the issue was not completely solved\",\n",
        "        \"Google Pixel 7's camera is excellent in low light.\",\n",
        "        \"Overall, a great product from Samsung.\"\n",
        "    ]\n",
        "    return random.choice(feedbacks)"
      ],
      "metadata": {
        "id": "1QtuJjgRNKqS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feedback_list = []\n",
        "\n",
        "for _ in range(5):\n",
        "    feedback_list.append(generate_feedback())\n",
        "\n",
        "# Tokenize using NLTK\n",
        "def nltk_tokenize(text):\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  words = []\n",
        "  for sentence in sentences:\n",
        "    words.extend(nltk.word_tokenize(sentence))\n",
        "  return sentences, words\n",
        "\n",
        "nltk_results = []\n",
        "for feedback in feedback_list:\n",
        "    nltk_results.append(nltk_tokenize(feedback))\n",
        "\n",
        "# Tokenize using spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "spacy_results = []\n",
        "for feedback in feedback_list:\n",
        "  doc = nlp(feedback)\n",
        "  spacy_tokens = [token.text for token in doc]\n",
        "  spacy_results.append(spacy_tokens)\n",
        "\n",
        "# Print the results\n",
        "for i, feedback in enumerate(feedback_list):\n",
        "     print(f\"Feedback {i+1}: {feedback}\")\n",
        "     print(\"NLTK Sentences:\", nltk_results[i][0])\n",
        "     print(\"NLTK Words:\", nltk_results[i][1])\n",
        "     print(\"spaCy Tokens:\", spacy_results[i])\n",
        "     print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n13QM32LZKx",
        "outputId": "95504538-faeb-47fa-e2fd-4015eede6667"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feedback 1: The phone's camera quality is amazing!\n",
            "NLTK Sentences: [\"The phone's camera quality is amazing!\"]\n",
            "NLTK Words: ['The', 'phone', \"'s\", 'camera', 'quality', 'is', 'amazing', '!']\n",
            "spaCy Tokens: ['The', 'phone', \"'s\", 'camera', 'quality', 'is', 'amazing', '!']\n",
            "--------------------\n",
            "Feedback 2: Customer service was helpful, resolved the issue quickly.\n",
            "NLTK Sentences: ['Customer service was helpful, resolved the issue quickly.']\n",
            "NLTK Words: ['Customer', 'service', 'was', 'helpful', ',', 'resolved', 'the', 'issue', 'quickly', '.']\n",
            "spaCy Tokens: ['Customer', 'service', 'was', 'helpful', ',', 'resolved', 'the', 'issue', 'quickly', '.']\n",
            "--------------------\n",
            "Feedback 3: Disappointed with the latest update, it's buggy.\n",
            "NLTK Sentences: [\"Disappointed with the latest update, it's buggy.\"]\n",
            "NLTK Words: ['Disappointed', 'with', 'the', 'latest', 'update', ',', 'it', \"'s\", 'buggy', '.']\n",
            "spaCy Tokens: ['Disappointed', 'with', 'the', 'latest', 'update', ',', 'it', \"'s\", 'buggy', '.']\n",
            "--------------------\n",
            "Feedback 4: Google Pixel 7's camera is excellent in low light.\n",
            "NLTK Sentences: [\"Google Pixel 7's camera is excellent in low light.\"]\n",
            "NLTK Words: ['Google', 'Pixel', '7', \"'s\", 'camera', 'is', 'excellent', 'in', 'low', 'light', '.']\n",
            "spaCy Tokens: ['Google', 'Pixel', '7', \"'s\", 'camera', 'is', 'excellent', 'in', 'low', 'light', '.']\n",
            "--------------------\n",
            "Feedback 5: Google Pixel 7's camera is excellent in low light.\n",
            "NLTK Sentences: [\"Google Pixel 7's camera is excellent in low light.\"]\n",
            "NLTK Words: ['Google', 'Pixel', '7', \"'s\", 'camera', 'is', 'excellent', 'in', 'low', 'light', '.']\n",
            "spaCy Tokens: ['Google', 'Pixel', '7', \"'s\", 'camera', 'is', 'excellent', 'in', 'low', 'light', '.']\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Removing Stopwords\n",
        "\n",
        "Customer feedback often contains common words like “the,” “is,” and “it” that do not provide much useful information. To focus on meaningful content, we can remove stopwords.\n",
        "\n",
        "a) **Remove Stopwords using NLTK** - Use NLTK’s list of stopwords to filter out\n",
        "irrelevant words.\n",
        "\n",
        "b) **Remove Stopwords using spaCy** (spaCy offers a built-in property (token.`is_stop`)\n",
        "to check whether a token is a stopword)"
      ],
      "metadata": {
        "id": "WwK4B-0EMO1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "# Remove stopwords using NLTK\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords_nltk(words):\n",
        "  return [word for word in words if word.lower() not in stop_words_nltk]\n",
        "\n",
        "# Remove stopwords using spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def remove_stopwords_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "# Print the results with stopwords removed\n",
        "for i, feedback in enumerate(feedback_list):\n",
        "    print(f\"Feedback {i+1}: {feedback}\")\n",
        "    print(\"NLTK Words (no stopwords):\", remove_stopwords_nltk(nltk_results[i][1]))\n",
        "    print(\"spaCy Tokens (no stopwords):\", remove_stopwords_spacy(feedback))\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2OQ8MSVMiWL",
        "outputId": "9eccdeb5-70d8-4d5f-e867-e2d5c801ab77"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feedback 1: The phone's camera quality is amazing!\n",
            "NLTK Words (no stopwords): ['phone', \"'s\", 'camera', 'quality', 'amazing', '!']\n",
            "spaCy Tokens (no stopwords): ['phone', 'camera', 'quality', 'amazing', '!']\n",
            "--------------------\n",
            "Feedback 2: Customer service was helpful, resolved the issue quickly.\n",
            "NLTK Words (no stopwords): ['Customer', 'service', 'helpful', ',', 'resolved', 'issue', 'quickly', '.']\n",
            "spaCy Tokens (no stopwords): ['Customer', 'service', 'helpful', ',', 'resolved', 'issue', 'quickly', '.']\n",
            "--------------------\n",
            "Feedback 3: Disappointed with the latest update, it's buggy.\n",
            "NLTK Words (no stopwords): ['Disappointed', 'latest', 'update', ',', \"'s\", 'buggy', '.']\n",
            "spaCy Tokens (no stopwords): ['Disappointed', 'latest', 'update', ',', 'buggy', '.']\n",
            "--------------------\n",
            "Feedback 4: Google Pixel 7's camera is excellent in low light.\n",
            "NLTK Words (no stopwords): ['Google', 'Pixel', '7', \"'s\", 'camera', 'excellent', 'low', 'light', '.']\n",
            "spaCy Tokens (no stopwords): ['Google', 'Pixel', '7', 'camera', 'excellent', 'low', 'light', '.']\n",
            "--------------------\n",
            "Feedback 5: Google Pixel 7's camera is excellent in low light.\n",
            "NLTK Words (no stopwords): ['Google', 'Pixel', '7', \"'s\", 'camera', 'excellent', 'low', 'light', '.']\n",
            "spaCy Tokens (no stopwords): ['Google', 'Pixel', '7', 'camera', 'excellent', 'low', 'light', '.']\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Extracting Named Entities\n",
        "\n",
        "In customer feedback, named entities such as product names or locations are important for analysis. You can use spaCy to identify named entities.\n",
        "\n",
        "a) **Extract Named Entities Using spaCy** - Modify your spaCy function to extract named entities (such as company names, products, or issues mentioned in the\n",
        "feedback)."
      ],
      "metadata": {
        "id": "Kwf1yMNeMyNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        entities.append((ent.text, ent.label_))\n",
        "    return entities\n",
        "\n",
        "# Print the results with named entities\n",
        "for i, feedback in enumerate(feedback_list):\n",
        "    print(f\"Feedback {i+1}: {feedback}\")\n",
        "    print(\"Named Entities:\", extract_entities(feedback))\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMD96j1yM9i3",
        "outputId": "33cbf30f-7d33-495b-8efe-de4621885c96"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feedback 1: The phone's camera quality is amazing!\n",
            "Named Entities: []\n",
            "--------------------\n",
            "Feedback 2: Customer service was helpful, resolved the issue quickly.\n",
            "Named Entities: []\n",
            "--------------------\n",
            "Feedback 3: Disappointed with the latest update, it's buggy.\n",
            "Named Entities: []\n",
            "--------------------\n",
            "Feedback 4: Google Pixel 7's camera is excellent in low light.\n",
            "Named Entities: [('Google', 'ORG'), ('Pixel', 'PERSON')]\n",
            "--------------------\n",
            "Feedback 5: Google Pixel 7's camera is excellent in low light.\n",
            "Named Entities: [('Google', 'ORG'), ('Pixel', 'PERSON')]\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Evaluating and Reflecting on Tokenization\n",
        "\n",
        "1. **Compare Tokenization Results** - Compare the results of NLTK and spaCy tokenization, and discuss their performance. Which method provides more\n",
        "accurate results? Which one is easier to use?\n",
        "2. ### Discussion\n",
        "  1. What are the strengths and weaknesses of NLTK and spaCy tokenization methods?\n",
        "  2. How did the removal of stopwords affect the tokenization results?\n",
        "  3. Did spaCy do a better job at extracting named entities? Why or why not?\n",
        "  4. How could you extend this process to analyze a larger set of customer feedback, such as identifying common themes or performing sentiment analysis?"
      ],
      "metadata": {
        "id": "x-fsKWYxN-vA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What are the strengths and weaknesses of NLTK and spaCy tokenization methods?\n",
        "\n",
        "NLTK offers a wider range of tokenization methods including word tokenization, sentence tokenization, and even regular expression-based tokenization. This makes it adaptable for different text processing scenarios.\n",
        "\n",
        "However, NLTK can be slower compared to spaCy, especially when processing large volumes of text. Additionally, it relies on rule-based methods and might not be as accurate as spaCy, which uses statistical models trained on large datasets.\n",
        "\n",
        "spaCy is known for its speed and efficiency in processing text, making it suitable for large datasets, but it can be resource-intensive, potentially requiring more memory and processing power.\n",
        "\n",
        "#### How did the removal of stopwords affect the tokenization results?\n",
        "\n",
        "The removal of stopwords eased the preparation process and subsequently now allows for the raw data to be consumed by natural language processing activities.\n",
        "\n",
        "#### Did spaCy do a better job at extracting named entities? Why or why not?\n",
        "\n",
        "No, because for smaller datasets that only use well-known entity names such as Google and Samsung, rule-based methods are enough.\n",
        "\n",
        "#### How could you extend this process to analyze a larger set of customer feedback, such as identifying common themes or performing sentiment analysis?\n",
        "\n",
        "We can use word-sense disambiguation (WSD) to understand the context behind each word, which necessarily requires tokenization.\n"
      ],
      "metadata": {
        "id": "NK8wpGn8OVNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Requirements:\n",
        "\n",
        "1. Submit the Python script containing all your implementations for tokenization,\n",
        "stopword removal, and named entity extraction.\n",
        "2. ### Provide answers to the following questions:\n",
        "  1. What are the advantages and limitations of NLTK and spaCy in text preprocessing?\n",
        "  2. How can tokenization help with analyzing customer feedback?\n",
        "  3. How does removing stopwords impact the analysis?\n",
        "  4. Why is it important to extract named entities from customer feedback?\n",
        "  5. What insights would you look for from tokenized feedback?"
      ],
      "metadata": {
        "id": "2xu77bfhOaji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What are the advantages and limitations of NLTK and spaCy in text preprocessing?\n",
        "\n",
        "NLTK offers a wider range of tokenization methods including word tokenization, sentence tokenization, and even regular expression-based tokenization. This makes it adaptable for different text processing scenarios.\n",
        "\n",
        "However, NLTK can be slower compared to spaCy, especially when processing large volumes of text. Additionally, it relies on rule-based methods and might not be as accurate as spaCy, which uses statistical models trained on large datasets.\n",
        "\n",
        "spaCy is known for its speed and efficiency in processing text, making it suitable for large datasets, but it can be resource-intensive, potentially requiring more memory and processing power.\n",
        "\n",
        "#### How can tokenization help with analyzing customer feedback?\n",
        "\n",
        "Tokenization can prepare words up to be absorbed by other activities done during natural language processing, such as word-sense disambiguation and sentiment analysis.\n",
        "\n",
        "#### How does removing stopwords impact the analysis?\n",
        "\n",
        "Removing stopwords helps improve the uniqueness of each word and cleans the list of any unnecessary parts of the sentence that don't contribute to further analysis.\n",
        "\n",
        "#### Why is it important to extract named entities from customer feedback?\n",
        "\n",
        "By identifying entities, we can link customer sentiments (positive or negative) to particular products, features, or aspects of their experience. For example, if many customers mention \"battery life\" with negative sentiment, it highlights a potential area for improvement.\n",
        "\n",
        "#### What insights would you look for from tokenized feedback?\n",
        "\n",
        "We can reveal common themes, topics and issues raised by customers. Aside from that, we can understand customers' overall sentiment towards each of our products, as well as segment them based on their interests and preferences."
      ],
      "metadata": {
        "id": "0m-yChROQvtN"
      }
    }
  ]
}