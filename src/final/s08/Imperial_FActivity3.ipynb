{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwxdYKxNGjf4"
   },
   "source": [
    "*De La Salle University – Dasmariñas* \\\n",
    "*College of Information and Computer Studies*\n",
    "\n",
    "**S–CSIS312LA — Natural Language Processing (Laboratory)** \\\n",
    "__Instructor:__ Josephine T. Eduardo\n",
    "\n",
    "**Name:** Luis Anton P. Imperial \\\n",
    "**CYS:** BCS32\n",
    "\n",
    "# Finals Activity 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu00jganGIzF"
   },
   "source": [
    "## Problem:\n",
    "\n",
    "A team of data scientists at a marketing firm wants to classify customer reviews into two categories: *Positive* and *Negative*. They are debating whether to remove stopwords from the text during the preprocessing phase of their NLP pipeline. The goal is to understand how stopword removal impacts the model's performance.\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. **Dataset:** A collection of 5,000 customer reviews from an e-commerce website.\n",
    "2. **Preprocessing:**\n",
    "  - **Model 1:** Removes stopwords using a predefined list from the NLTK library.\n",
    "  - **Model 2:** Does not remove stopwords, keeping all words for classification.\n",
    "  - **Evaluation Metrics:** Precision, recall, and F1-score.\n",
    "\n",
    "## Results\n",
    "\n",
    "- **Model 1 (with stopword removal):** Achieved a precision of 0.85, recall of 0.83, and F1-score of 0.84.\n",
    "- **Model 2 (without stopword removal):** Achieved a precision of 0.83, recall of 0.81, and F1-score of 0.82.\n",
    "\n",
    "## Analysis\n",
    "\n",
    "- The removal of stopwords helped the model achieve slightly better performance in terms of precision, recall, and F1-score. The model’s focus was more on the sentiment-bearing words, which are more indicative of positive or negative reviews.\n",
    "- The slight improvement in performance was due to the reduced dimensionality of the feature space and the focus on content-heavy words.\n",
    "- However, removing stopwords did not lead to a dramatic improvement, indicating that for this specific task, the impact of stopword removal was moderate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3X_21FrF-Ac",
    "outputId": "870c5a4a-3c2a-46c2-c296-0e9048423721"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Q630gjQ5Ip1R"
   },
   "outputs": [],
   "source": [
    "dataset = \"Imperial_FActivity3_Reviews.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LARlF7oHjy2",
    "outputId": "cc8669d9-dc3e-42ba-b80a-a6fd1a93bfb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                           Wow... Loved this place.      1\n",
      "1                                 Crust is not good.      0\n",
      "2          Not tasty and the texture was just nasty.      0\n",
      "3  Stopped by during the late May bank holiday of...      1\n",
      "4  The selection on the menu was great and so wer...      1\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(dataset, sep=\"\\t\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "amsMHa1FJQew",
    "outputId": "9529bf56-01b2-49c8-d5aa-bd0758e6a3f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lpimp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return \" \".join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bINWgqmJJoBK",
    "outputId": "13007d5b-add0-4383-d649-93abb4cd5b57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\lpimp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked  \\\n",
      "0                           Wow... Loved this place.      1   \n",
      "1                                 Crust is not good.      0   \n",
      "2          Not tasty and the texture was just nasty.      0   \n",
      "3  Stopped by during the late May bank holiday of...      1   \n",
      "4  The selection on the menu was great and so wer...      1   \n",
      "\n",
      "                                 Review_No_Stopwords  \n",
      "0                              Wow ... Loved place .  \n",
      "1                                       Crust good .  \n",
      "2                              tasty texture nasty .  \n",
      "3  Stopped late May bank holiday Rick Steve recom...  \n",
      "4                      selection menu great prices .  \n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Create a new column with stopwords removed\n",
    "df['Review_No_Stopwords'] = df['Review'].apply(remove_stopwords)\n",
    "\n",
    "# Display the first few rows with the new column\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ntui0ajhKT34",
    "outputId": "3da22bca-be15-4206-96a2-1e323f8bef6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with Stopwords Removed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.85      0.78        96\n",
      "           1       0.84      0.69      0.76       104\n",
      "\n",
      "    accuracy                           0.77       200\n",
      "   macro avg       0.78      0.77      0.77       200\n",
      "weighted avg       0.78      0.77      0.77       200\n",
      "\n",
      "\n",
      "Model with Stopwords Retained:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.85      0.80        96\n",
      "           1       0.85      0.74      0.79       104\n",
      "\n",
      "    accuracy                           0.80       200\n",
      "   macro avg       0.80      0.80      0.79       200\n",
      "weighted avg       0.80      0.80      0.79       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prepare the data for model training\n",
    "X_with_stopwords = df['Review']\n",
    "y = df['Liked']  # Assuming 'Liked' column represents sentiment labels (0 or 1)\n",
    "\n",
    "X_without_stopwords = df['Review_No_Stopwords']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_with, X_test_with, y_train_with, y_test_with = train_test_split(\n",
    "    X_with_stopwords, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_without, X_test_without, y_train_without, y_test_without = train_test_split(\n",
    "    X_without_stopwords, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and train the models\n",
    "\n",
    "# Model 1 (with stopwords)\n",
    "vectorizer_with = TfidfVectorizer()\n",
    "X_train_vec_with = vectorizer_with.fit_transform(X_train_with)\n",
    "X_test_vec_with = vectorizer_with.transform(X_test_with)\n",
    "\n",
    "model_with = LogisticRegression()\n",
    "model_with.fit(X_train_vec_with, y_train_with)\n",
    "y_pred_with = model_with.predict(X_test_vec_with)\n",
    "\n",
    "\n",
    "# Model 2 (without stopwords)\n",
    "vectorizer_without = TfidfVectorizer()\n",
    "X_train_vec_without = vectorizer_without.fit_transform(X_train_without)\n",
    "X_test_vec_without = vectorizer_without.transform(X_test_without)\n",
    "\n",
    "model_without = LogisticRegression()\n",
    "model_without.fit(X_train_vec_without, y_train_without)\n",
    "y_pred_without = model_without.predict(X_test_vec_without)\n",
    "\n",
    "# Evaluate the models\n",
    "\n",
    "print(\"Model with Stopwords Removed:\")\n",
    "print(classification_report(y_test_without, y_pred_without))\n",
    "\n",
    "print(\"\\nModel with Stopwords Retained:\")\n",
    "print(classification_report(y_test_with, y_pred_with))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
