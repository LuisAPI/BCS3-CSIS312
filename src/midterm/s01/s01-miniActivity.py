# -*- coding: utf-8 -*-
"""s01-miniActivity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G2tnWPJrzJBqJ3LBbupq7CtPjthE-yt0

## 240822 - Mini Activity

- Type: Discussion
- Max score: 100
- Category: Class Participation
- Start: Aug 22, 3:00 pm
- Due: Sep 7, 6:00 pm
- Allow late submissions: ‚ùå

Please refer to the Lesson and Discussion for the instruction.

Screenshot your code and result and write your insights.

First 5 submitted without error - 100  
Second 5 submitted without error - 90  
The rest of submission without error - 85  
Late submission without error - 75  
No submission and with an error is 0
"""

import nltk
import spacy
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from spacy.lang.en import English
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline

nltk.download('punkt')
nltk.download('stopwords')

nlp = English()

texts = [
    "The movie was fantastic, I loved every moment of it",
    "The food was terrible, I would never eat there again",
    "I had a great time at the concert",
    "The service at the restaurant was horrible",
    "I really enjoyed the book",
    "The hotel room was dirty and uncomfortable",
    "I am very satisfied with my purchase",
    "The delivery was late and the package was damaged",
    "The customer support was very helpful",
    "I am disappointed with the quality of the product"
         ]
labels = ['Positive',
          'Negative',
          'Positive',
          'Negative',
          'Positive',
          'Negative',
          'Positive',
          'Negative',
          'Positive',
          'Negative'
]

new_texts_to_analyze = 10

model1 = make_pipeline(CountVectorizer(), MultinomialNB())
model1.fit(texts, labels)

for statement in texts:
    print("Text to analyze:", statement)
    token = word_tokenize(statement)
    print("Tokenized text:", token)

    prediction = model1.predict([statement])
    print(f"Predicted sentiment using CountVectorizer:", prediction[0], end="\n\n")

i = 0
while i < new_texts_to_analyze:
  user_input = input("Enter a text: ")
  if user_input == "no":
    break

  prediction = model1.predict([user_input])
  print("Predicted Sentiment using CountVectorizer:", prediction[0])
  i += 1

model2 = make_pipeline(TfidfVectorizer(), MultinomialNB())
model2.fit(texts, labels)

for statement in texts:
    print("Text to analyze:", statement)
    token = word_tokenize(statement)
    print("Tokenized text:", token)

    prediction = model2.predict([statement])
    print(f"Predicted sentiment using TfidfVectorizer:", prediction[0], end="\n\n")

print("-----")
print("NEW TEXTS FOLLOW")
print("-----")

i = 0
while i < new_texts_to_analyze:
  user_input = input("Enter a text: ")
  if user_input == "no":
    break

  prediction = model2.predict([user_input])
  print("Predicted Sentiment using TdidfVectorizer:", prediction[0], end="\n\n")
  i += 1